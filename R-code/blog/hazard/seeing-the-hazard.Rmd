---
title: "Seeing the hazard"
author: "Ben Ogorek"
header-includes:
   - \usepackage{bbm}
date: "May 19, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Introduction

### Distributions vs Hazards 

Distributions are hard things to love. They admit uncertainty.
They're effort beyond the best guess from a 
machine learning model. They involve often dubious assumptions, and that's
often the least of the concerns considering
[people don't understand them anyways.](https://www.inverse.com/article/6339-why-people-suck-at-understanding-probability)

But in the case of *when* a potentially unpleasant event might occur,
a little uncertainty goes a long way. And if the density and
distribution functions of the time-to-event variable,
$f(t)$ and $F(t)$,
still don't resonate, the time-to-event situtation
has its own instrument of uncertainty, the
**hazard**, with the unavoidable interpretation of *instantaneous
risk through time*:

$$
h(t) = f(t) / (1 - F(t)), t \in [0, \infty).
$$

Clearly the hazard contains the same information as the density, but
its interpretation as an instantaneous risk (for those still suceptible)
makes it an especially useful tool for regression modeling. If you've ever cringed at
a statement about the number of minutes each
serving of product Y takes off your life,
then you're familiar with the awkwardness of
ordinary regression with a time-to-event variable. 
If consumption of Product Y was such a bad idea that it 
[doubled your risk from the event in question](http://www.cnn.com/2017/06/14/health/fried-potatoes-early-death/index.html)
at all points in time, that's more palatable.


```{r echo=FALSE}

shape <- 1.2 # try with 1, 1.5
scale <- 5 

t_seq <- seq(0, 15, .1)
density <- dweibull(t_seq, shape, scale)
hazard <- density / (1 - pweibull(t_seq, shape, scale))
# hazard <- scale ^ (-shape) * shape * t_seq ^ (shape - 1) # Same as above!
plot(density ~ t_seq, type = "l", col = "blue", lwd = 2, xlab = "t",
     ylim = c(0, 1.5 * max(hazard)), ylab = "density (blue) or hazard (red)",
     main = "Proportional hazards and their corresponding densities",
     cex.axis = 1.2, cex.lab = 1.4, cex.main = 1.3)
lines(hazard ~ t_seq, type = "l", col = "red", lwd = 2)

hazard_b <- hazard * 1.3
scale_b <- scale * (1.3) ^ (-1 / shape)
density_b <- dweibull(t_seq, shape, scale_b) 
# hazard_b <- density_b / (1 - pweibull(t_seq, shape, scale_b)) same as above!
lines(hazard_b ~ t_seq, type = "l", lty = 2, col = "red", lwd = 2)
lines(density_b ~ t_seq, type = "l", lty = 2, col = "blue", lwd = 2)
```

The above figure illustrates the relationship between the hazard (red) and
the density (blue), with the dotted lines corresponding to a hazard
that has been multiplied at all points by 1.3. Since this second hazard is
proportionally higher at all
points in time, its corresponding density has more mass
concentrated around smaller values of $t$. 
Both situations imply the same *aging* mechanism:
the risk increases quickly at first, and then less quickly
as time goes on.

### Hazard regression
To turn this into a regression problem, we need a way to modulate the positive
multiplier on the hazard as a function of covariate vector $\mathbf{x}$.
Using a linear
approximation, one way is to put $\mathbf{x}^T \beta$
in the exponent of a positive number. The most popular positive number to
exponentiate surely is $e$, leading to the proportional hazards regression model
$$
h(t; \mathbf{x}, \beta) = \lambda_0(t) \exp(\mathbf{x}^T \mathbf{\beta}),
$$

where $\lambda_0(t)$ is the "baseline" hazard function that is proportionally
raised or lowered depending on the values of the covariate $\mathbf{x}$ and parameter $\beta$.

The next question is where to get $\lambda_0(t)$. A weibull model
offers convenient parametric forms for both hazard and density, but
does not offer much flexibility in the hazard's shape.
For instance, one shape that you might expect to see is a "bathtub," where
the risk is high at early ages, low during the middle of life, and then
increasing again in later life. The weibull model can't even accomodate that,
 and the likely complexity of any real $\lambda_0(t)$ 
is why D. R. Cox's ([1972](#references)) result, allowing valid statistical
inferences about $\beta$
without any work to estimate $\lambda_0(t),$ is still so celebrated today.

### Losing lambda

On the other hand, $\lambda_0(t)$ may contain useful information about the
failure mechanism at hand. But rather than estimating it directly, nearly
every survival analysis tutorial on the web
focuses on estimating $H(t) = \int_0^t \lambda_0(t) dt$, the "cumulative
hazard" rather than the more interpretable $\lambda_0(t)$.
Bayesian nonparametrician [Hjort (1990)](#references) sheds some light on this:
*"Now [the hazard] is as difficult to estimate with good precision as is the
density $F^{\prime}(t)$, so we prefer working with with the cumulative 
hazard [emphasized]...The cumulative hazard can be defined even
when $F$ has no density."*

Cox's semiparametric method may cleverly avoid inference on $\lambda_0(t)$ 
in favor of $\beta$, and $\lambda_0(t)$ may not be especially convenient to
work with,
but in this article we're going to *see* it anyway. 
The plan is straightforward: apply smoothing to and then take an
approximate derivative of the cumulative hazard.
But first we will simulate from a baseline hazard that will be 
absolutely unmistakable if we're able to recover it: a sinusoid!

## Simulation of a sinusoidal hazard

Beyond R, there's likely nothing to be installed packages for this tutorial,
as we'll be depending on `survival` and `splines` packages (a *recommended*
and *base* package, respectively).
```{r}
library(survival)
library(splines)
```

This simulation uses a sample size of $N = 1000$ and 
two independent, normally distributed covariates, $x_1$ and $x_2$.

```{r}
N <- 1000
set.seed(14323)
sim_data <- data.frame(x1 = rnorm(N), x2 = rnorm(N))
```

Despite being continuous-time at heart, inside of a computer it is
necessary to discretize time. The smaller the value of `delta`, the closer
the approximation to continuous time.

```{r}
t_max_for_sim <- 25 # large enough to exceed last event (censor or death)
delta <- .01
t_seq <- seq(0, t_max_for_sim, delta)
```

Below is the baseline hazard as promised:
a sinusoidal function with a bias term and a period of 12.
Not exactly a "bathtub"!

```{r}
baseline_hazard <- .1 * (1.1 + sin(2 * pi * t_seq / 12))
plot(baseline_hazard ~ t_seq,
     main = "The baseline hazard", cex.main = 1.6, col = "grey35",
     xlab = "t", yaxt = "n", ylab = "", cex.axis = 1.5, cex.lab = 1.6)
axis(2, cex.axis = 1.2)
mtext(expression(lambda[0](t)), side = 2, line = 2.2, cex = 2)
```


The following function simulates an observed lifetime given covariate values
`x1` and `x2`, the baseline hazard (`lambda_0`) sampled over a time grid
with equidistant spacing `dt`, and `dt` itself.
The values of $\beta$ corresponding to `x1` and `x2`, 
.1 and -.1, respectively, are hard-coded into the formula.
A $Gamma(25, 2)$
random variable is used as the max observable time; if the event time
exceeds this value, then it will be "censored" and the max observable
time is recorded in its place. Thus the return value is a bivariate vector,
the first
element being the random event time and the second the censoring indicator.

The event times (before considering censoring) can be generated directly from the 
hazard via its definition as an infinitesimal quantity:
$$
h(t) = \lim_{\delta \rightarrow 0} \frac{Pr(T \in [t, t + \delta) | T \ge t)}{\delta},
$$
since for small $\delta$, 
$$
\delta \cdot h(t) \approx Pr(T \in [t, t + \delta) | T \ge t).
$$
Thus, one may simply traverse the intervals of the $\delta$-spaced grid,
questioning whether a $\mathit{Uniform}(0, 1)$ random variable falls less than
$\delta$ times the hazard at that grid point.
```{r}
simulate_lifetime <- function(x1, x2, lambda_0, dt) {
  i <- 0
  alive <- TRUE
  censored <- FALSE
  t_max_observable <- rgamma(1, 25, 2)
  while(alive & !censored) {
    i <- i + 1
    pr_die_in_interval <- dt * lambda_0[i] * exp(.1 * x1 - .1 * x2)
    alive <- !(runif(1) <= pr_die_in_interval)
    censored <- t_seq[i] >= t_max_observable
  }
  return(c(t_seq[i], censored))
}
```

The final step in the simulation is to apply `simulate_lifetime` to every row
of `sim_data` to finish our simulation.
(Recall that the latter two passed parameters exist
in the global environment.)
```{r}
dat <- sapply(1:N, FUN = function(i) with(sim_data[i, ],
                                          simulate_lifetime(x1, x2,
                                                            baseline_hazard,
                                                            delta)))
sim_data <- as.data.frame(cbind(sim_data, t(dat)))
names(sim_data) <- c("x1", "x2", "t", "censored")
head(sim_data)
```

## Recovering the hazard

Applying the Cox Proportional Hazards model using the flagship `coxph`
function from the `survival` package is quite convenient. Compare the estimates of
$\beta_1$ and $\beta_2$ to .1 and -.1 respectively.
```{r}
my_model <- coxph(Surv(time = t, event = !censored, type = 'right') ~ x1 + x2,
                  data = sim_data)
summary(my_model)
```

Having $\hat{\beta}$ already is great, but
we want that sinusoidal back. The `coxph` function will make us 
work for it, but its output gives us everything we need.
```{r}
baseline_surv <- survfit(my_model)

surv_df <- data.frame(H = baseline_surv$cumhaz,
                      t = baseline_surv$time)
```
As tempting as it might be to simply take a difference of `H` as is,
note that the intervals between points are of different sizes. Additionally,
since `H` is constant between jumps, its derivative will be discontinuous,
alternating from zero to the size of the jump.

To get a continuous derivative, we need a sufficiently smooth function. And while
there's definitely
more than one way to do this, the parametric spline-based technique used below is fast
and easily allows interpolation onto an equally spaced grid. The `df`
parameter controls the amount of smoothing, and the reader is encouraged
to try higher and lower values while evaluating the resulting fit.

```{r}
surv_spline <- lm(H ~ ns(t, df = 12), data = surv_df)
plot(surv_df$H ~ surv_df$t, main = "The cumulative hazard with spline fit",
     ylab = "H(t)", xlab = "t", cex.lab = 1.4)
lines(surv_spline$fitted ~ surv_df$t, col = "red", lwd = 1.2)
```

It's time to see the hazard. First we'll use our spline representation of
the cumulative hazard to interpolate $\hat{H}(t)$ along our $\delta$-spaced
grid. Next we difference the smoothed estimate and divide by the length
of the interval ($\delta$) to get the approximate derivative.
```{r}
grid_df <- data.frame(t = t_seq)
grid_df$H <- predict(surv_spline, newdata = grid_df)

plot(diff(grid_df$H) / delta ~ t_seq[-1], xlim = c(0, 18),
     main = "Smoothed hazard estimate vs truth", cex.main = 1.8, col = "grey40",
     xlab = "t", yaxt = "n", ylab = "", cex = .8, cex.axis = 1.4, cex.lab = 1.4)
axis(2, cex.axis = 1.2)
mtext(expression(hat(lambda[0])(t)), side = 2, line = 2.0, cex = 1.5)
lines(baseline_hazard[-1] ~ t_seq[-1], col = "red", lwd = 1.3)
```

The true baseline hazard is seen above in red. Our estimate is not perfect,
but we've recovered the basic sinusoidal shape with only 1000 observations,
roughly a quarter of which have been censored.

# Discussion

In this article, we have combined parametric spline-based smoothing of the cumulative
hazard estimate with approximate differentiation to visualize an
unconventional baseline hazard function. The method is not free from 
tuning (e.g., the degree of smoothing) but we were able to recover the general
shape of the hazard function by visually assessing the fit of the spline to the
cumulative hazard.

Clearly there are other options for smoothing beyond
a parametric spline with a tunable number of
of "degrees of freedom."
Kernel methods stand out as an alternative, though 
they depend on a bandwidth parameter. 
Smoothing splines (with knots at every data point and shrinkage determined by
cross validation) are another alternative,
however in this author's informal experiments,
`stats::smooth.spline` did not result in sufficient
smoothing when applied to the cumulative hazard.

In a typical application of the Cox proportional hazards model,
the regression coefficients take center stage. While
Cox's (1972)
method allows regression
parameter inference without modeling an infinitely dimensional 
latent function,
a more complete understanding of the time-to-event
phenomenon under study is possible by visualizing the hazard. Though
not as automatic as it's cumulative counterpart, seeing the hazard
is quickly achievable with
R's smoothing tools and `survival` library. 

# References

Cox, D. R. 1972. "Regression Models and Life Tables (with Discussion)." Journal of the Royal Statistical
Society, Series B 34:187-220.

Hjort, N. (1990). Nonparametric Bayes estimators based on beta processes in models for life history data. Annals of Statistics 18, 1259-1294.
