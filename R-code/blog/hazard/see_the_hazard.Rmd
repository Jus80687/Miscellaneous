---
title: "Watch for that hazard!"
author: "Ben Ogorek"
header-includes:
   - \usepackage{bbm}
date: "May 19, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Distributions vs Hazards 

Distributions are hard things to love. They admit uncertainty.
Discovering them is extra work beyond a "best guess" from an expectation or 
machine learning model. They involve often dubious assumptions and 
they can [look ridiculous in hindsight](https://projects.fivethirtyeight.com/2016-election-forecast/).

On the other hand, there's something
fundamental about uncertainty in *when* a potentially unpleasant event might
occur. True, the hazard is merely an alternative representation of a lifetime
distribution, 
but this author has met more than one analyst, lacking formal statistical training,
that really wanted to work with hazards. 
While [Discrete time-to-event analysis](https://statisticalhorizons.com/wp-content/uploads/Allison.SM82.pdf),
framable in the context of logistic regression, might be a better starting
point, 
These analysts want the real thing:
Cox's celebrated proportional hazards model ([Cox 1972](#references)),
where the hazard for an individual is modeled as:
$$
h(t; \mathbf{x}_i) = \lambda_0(t) \exp(\mathbf{x}_i^T \beta),
$$

$\lambda_0(t)$ being the continuous "baseline" hazard function, $\mathbf{x}_i$
the vector of individual-level covariates, and $\beta$ is a vector of regression
coefficients.
Subscribing to the proportional hazards assumption and limiting attention
to stable covariates through time, 
$\lambda_0(t)$ offers
an understanding of the instantaneous risk through time. Does it have
a bathtub shape? Is it always increasing? Or is it mostly flat?

These are interesting and useful questions that shed light on the failure
mechanism. On the other hand, nearly every survival analysis walkthrough
focuses on estimating $H(t) = \int_0^t \lambda_0(t) dt$, the "cumulative
hazard", the cases being closed without ever investigating $\lambda_0(t)$.
Bayesian nonparametrician [Hjort (1990)](#references) sheds some light on this:
> "Now [the hazard] is as difficult to estimate with good precision as is the
> density $F^{\prime}(t)$, so we prefer working with with the *cumulative 
> hazard*...The cumulative hazard can be defined even when $F$ has no density."

So the cumulative counterpart of $\lambda_0(t)$ is much
easier to deal with. What also doesn't help  $\lambda_0(t)$'s visibility
is that Cox's 1972 celebrated semiparametric method 
allows quick and easy inference on $\beta$ alone;
$\lambda_0(t)$ becomes a "nuisance," as inference on which has been
cleverly avoided.

Regardless, in this article we're going to *see* the hazard. 
It won't be rocket science; we'll apply smoothing and then take an approximate
derivative of the cumulative hazard.
But first we will simulate from a baseline hazard that will be 
absolutely unmistakable if we're able to recover it: a sinusoid!

## Simulation

Beyond R, there's likely nothing to be installed packages for this tutorial,
as we'll be depending on `survival` and `splines` packages (a *recommended*
and *base* package, respectively).
```{r}
library(survival)
library(splines)
```

This simulation uses a sample size of $N = 1000$ and 
two independent, normally distributed covariates, $x_1$ and $x_2$.

```{r}
N <- 1000
set.seed(14323)
sim_data <- data.frame(x1 = rnorm(N), x2 = rnorm(N))
```

Despite being continuous-time at heart, inside of a computer it is
necessary to discretize time. The smaller the value of $delta$, the closer
the approximation to continuous time.

```{r}
t_max_for_sim <- 25 # large enough to exceed last event (censor or death)
delta <- .01
t_seq <- seq(0, t_max_for_sim, delta)
```

Below is the baseline hazard as promised:
a sinosoidal function with a bias term and a period of 12.
Not exactly a "bathtub"!

```{r}
baseline_hazard <- .1 * (1.1 + sin(2 * pi * t_seq / 12))
plot(baseline_hazard ~ t_seq,
     main = "The baseline hazard", cex.main = 1.6, col = "grey35",
     xlab = "t", yaxt = "n", ylab = "", cex.axis = 1.5, cex.lab = 1.6)
axis(2, cex.axis = 1.2)
mtext(expression(lambda[0](t)), side = 2, line = 2.2, cex = 2)
```


The following function simulates an observed lifetime given covariate values
`x1` and `x2`, the baseline hazard (`lambda_0`) sampled over a time grid
with equidistant spacing `dt`, and `dt` itself.
The values of $\beta$ corresponding to `x1` and `x2`, 
.1 and -.1, respectively, are hard-coded into the formula.
A $Gamma(25, 2)$
random variable is used as the max observable time; if the event time
exceeds this value, then it will be "censored" and the max observable
time is recorded in its place. Thus the return value is a bivariate vector,
the first
element being the random event time and the second the censoring indicator.

The event times (before considering censoring) can be generated directly from the 
hazard via its definition as an infinitesimal quantity:
$$
h(t) = \lim_{\delta \rightarrow 0} \frac{Pr(T \in [t, t + \delta) | T \ge t)}{\delta},
$$
since for small $\delta$, 
$$
\delta \cdot h(t) \approx Pr(T \in [t, t + \delta) | T \ge t).
$$
one may simply traverse the intervals of the $\delta$-spaced grid,
questioning whether a $\mathit{Uniform}(0, 1)$ random variable falls less than
$\delta$ times the hazard at that grid point.
```{r}
simulate_lifetime <- function(x1, x2, lambda_0, dt) {
  i <- 0
  alive <- TRUE
  censored <- FALSE
  t_max_observable <- rgamma(1, 25, 2)
  while(alive & !censored) {
    i <- i + 1
    pr_die_in_interval <- dt * lambda_0[i] * exp(.1 * x1 - .1 * x2)
    alive <- !(runif(1) <= pr_die_in_interval)
    censored <- t_seq[i] >= t_max_observable
  }
  return(c(t_seq[i], censored))
}
```

The final step in the simulation is applying the above function to every row
of `sim_data` to finish our simulation.
Recall that the latter two parameters passed to `simulated_lifetime` exist
in the global environment.
```{r}
dat <- sapply(1:N, FUN = function(i) with(sim_data[i, ],
                                          simulate_lifetime(x1, x2,
                                                            baseline_hazard,
                                                            delta)))
sim_data <- as.data.frame(cbind(sim_data, t(dat)))
names(sim_data) <- c("x1", "x2", "t", "censored")
```

## Recovering the hazard

Applying the Cox Proportional Hazards model using the flagship `coxph`
function from the `survival` package is quite convenient. Compare the estimates of
$\beta_1$ and $\beta_2$ to .1 and -.1 respectively.
```{r}
my_model <- coxph(Surv(time = t, event = !censored, type = 'right') ~ x1 + x2,
                  data = sim_data)
summary(my_model)
```

But we're not satisfied with $\hat{\beta}$,
we want that sinusoidal back. The `coxph` function will make us 
work for it, but its output gives us everything we need.
```{r}
baseline_surv <- survfit(my_model)

surv_df <- data.frame(H = baseline_surv$cumhaz,
                      t = baseline_surv$time)
```
As tempting as it might be to simply take a difference of `H`,
the intervals between points are different, so that won't work. Additionally,
since `H` is constant between jumps, its derivative will be discontinuous,
alternating from zero to the size of the jump.

To get a continuous derivative, we need a sufficiently smooth function. And while
there's definitely
more than one way to do this, the spline-based technique used below is fast
and easily allows interpolation onto an equally spaced grid. The `df`
parameter controls the amount of smoothing, and the reader is encouraged
to try higher and lower values while evaluating the resulting fit.

```{r}
surv_spline <- lm(H ~ ns(t, df = 12), data = surv_df)
plot(surv_df$H ~ surv_df$t, main = "The cumulative hazard with spline fit",
     ylab = "H(t)", xlab = "t", cex.lab = 1.4)
lines(surv_spline$fitted ~ surv_df$t, col = "red", lwd = 1.2)
```

It's time to see the hazard. First we'll use our spline representation of
the cumulative hazard to interpolate $\hat{H}(t)$ along our $\delta$-spaced
grid. Next we difference and divide by the length
of the interval ($\delta$) to get the approximate derivative. Finally, we
overly the true baseline hazard in red. Our estimate is not perfect,
but we've recovered the basic sinusoidal shape with only 1000 observations,
roughly a quarter of which have been censored.
```{r}
grid_df <- data.frame(t = t_seq)
grid_df$H <- predict(surv_spline, newdata = grid_df)

plot(diff(grid_df$H) / delta ~ t_seq[-1], xlim = c(0, 18),
     main = "Smoothed hazard estimate vs truth", cex.main = 1.8, col = "grey40",
     xlab = "t", yaxt = "n", ylab = "", cex = .8, cex.axis = 1.4, cex.lab = 1.4)
axis(2, cex.axis = 1.2)
mtext(expression(hat(lambda[0])(t)), side = 2, line = 2.0, cex = 1.5)
lines(baseline_hazard[-1] ~ t_seq[-1], col = "red", lwd = 1.3)
```

# Discussion

In this article, we have combined spline-based smoothing of the cumulative
hazard estimate with approximate differentiation to visualize an
unconventional baseline hazard function. The method is not free from arbitrary
choices (e.g., the degree of smoothing) but we were able to recover the general
shape of the hazard function by visually assessing the fit of the spline to the
cumulative hazard.

Clearly there are other options for smoothing beyond
a parametric spline with an arbitrary number (not on *this* blog!)
of "degrees of freedom."
Kernel methods stand out as an alternative, though 
they depend on an equally arbitrary bandwidth parameter. 
Smoothing splines (with knots at every data point and shrinkage determined by
cross validation) are another alternative,
however in this author's informal experiments,
`stats::smooth.spline` did not result in sufficient
smoothing when applied to the cumulative hazard.

In the typical application of the Cox proportional hazards model,
the regression coefficients take center stage. Undoubtedly, Cox's (1972) seminal
result allowng regression
parameter inference without modeling a potentially
complicated continuous-time latent function, is a monumental achievement.
But a more complete understanding of the time-to-event
phenomenon under study is possible by visualizing the hazard, which, while
not as automatic as it's cumulative counterpart, is quickly achievable with
R's smoothing tools and `survival` library. 

# References

Cox, D. R. 1972. "Regression Models and Life Tables (with Discussion)." Journal of the Royal Statistical
Society, Series B 34:187-220.

Hjort, N. (1990). Nonparametric Bayes estimators based on beta processes in models for life history data. Annals of Statistics 18, 1259-1294.
